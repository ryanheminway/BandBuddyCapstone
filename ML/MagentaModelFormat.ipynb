{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "insured-equilibrium",
   "metadata": {},
   "source": [
    "# Magenta Model Format Investigation\n",
    "\n",
    "In the `MagentaDemo.ipynb` notebook, we demonstrate Magenta's libraries and a pre-trained model. For this model (or similar) to work in our hardware environment, we need the ability to convert it to a TensorFlow-Lite model. Magenta defines their own definition of what a \"model\" is by their \"TrainedModel\" class, so this conversion is not as straight forward as we may desire. This notebook is a sandbox to experiment with methods of converting the Magenta model to a TensorFlow model which can be easily converted to a TensorFlow-Lite.\n",
    "\n",
    "For the following code to successfuly run, you will need to follow the setup listed in `MagentaDemo.ipynb`\n",
    "\n",
    "I found this all by digging through https://github.com/magenta/magenta/tree/2d0fd456d7faa272733b57d286f5f26998082cf8/magenta/models/music_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS JUST REALLY FOR SETUP. INITIALIZES TRAINED MODEL AND SETS UP LIBRARIES. \n",
    "# MUST BE RUN FOR ANY OF THE FOLLOWING CELLS TO RUN\n",
    "\n",
    "#@title Setup Environment and Define all helper functionality\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# General / Math / Sound libraries\n",
    "import copy, warnings, librosa, numpy as np\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Magenta specific stuff\n",
    "import magenta\n",
    "from magenta.models.music_vae import configs\n",
    "from magenta.models.music_vae.trained_model import TrainedModel\n",
    "from magenta.models.music_vae import data\n",
    "\n",
    "# Disable \"eager execution\". Some errors indicate this is necessary, but this doesn't actually fix them :( \n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Disable TF 2.0 behavior. Seems like most of Magenta's stuff is based on 1.0 behavior.\n",
    "# This seems like the appropriate thing to do, but unfortunately does not fix any errors\n",
    "#tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "# Load model checkpoint\n",
    "GROOVAE_2BAR_TAP_FIXED_VELOCITY = \"model_checkpoints/groovae_2bar_tap_fixed_velocity.tar\"\n",
    "config_2bar_tap = configs.CONFIG_MAP['groovae_2bar_tap_fixed_velocity']\n",
    "# Create a TrainedModel (Magenta class) from config and checkpoint\n",
    "# The config specifies the type of the model, and their TrainedModel class constructs the\n",
    "#   appropriate back-end tensorflow graph for that specific model\n",
    "groovae_2bar_tap = TrainedModel(config_2bar_tap, 1, checkpoint_dir_or_path=GROOVAE_2BAR_TAP_FIXED_VELOCITY)\n",
    "\n",
    "print(\"CONFIG: \", config_2bar_tap)\n",
    "print(\"\\n\")\n",
    "print(\"CONFIG HPARAMS: \", config_2bar_tap.hparams)\n",
    "print(\"\\n\")\n",
    "print(\"ENCODER RNN SIZE: \", config_2bar_tap.hparams.enc_rnn_size)\n",
    "print(groovae_2bar_tap) # magenta.models.music_vae.trained_model.TrainedModel object\n",
    "print(groovae_2bar_tap._sess) # tensorflow.python.client.session.Session object\n",
    "print(groovae_2bar_tap._sess.graph) \n",
    "print(groovae_2bar_tap._sess.graph.get_collection('saveable_objects')) # tensorflow.python.client.session.Session object\n",
    "\n",
    "#print(groovae_2bar_tap._inputs)\n",
    "#print(groovae_2bar_tap._outputs)\n",
    "#print(tf.compat.v1.global_variables()) # nothing\n",
    "#print(config_2bar_tap.model) # magenta.models.music_vae.base_model.MusicVAE\n",
    "#print(config_2bar_tap.model.encoder) # magenta.models.music_vae.lstm_models.BidirectionalLstmEncoder \n",
    "#print(config_2bar_tap.model.encoder._cells) # THIS GIVES AN ERROR.... BUT WHY???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-future",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DESCRIPTION OF APPROACH (RYAN):\n",
    "#    I found I can actually access the TrainedModel \"session\" and \"graph\" attributes directly from the TrainedModel object.\n",
    "#    In theory, I thought this should be enough to translate directly to TFLite. But it's not working.\n",
    "#    Depending on whether \"eager execution\" is enabled at the top of the first cell, I get different errors. \n",
    "#    Input / output shapes are based on what I find when I access [TrainedModel]._input and [TrainedModel]._output\n",
    "\n",
    "# Try to export to SavedModel from TrainedModel session\n",
    "\n",
    "# print(groovae_2bar_tap._sess.graph.get_operations())\n",
    "print(groovae_2bar_tap._inputs)\n",
    "print(groovae_2bar_tap._outputs)\n",
    "new_input = tf.compat.v1.placeholder(tf.float32, shape=[1, None, 27]) \n",
    "new_output = tf.compat.v1.placeholder(tf.float32, shape=[1, None, 27])\n",
    "converter = tf.compat.v1.lite.TFLiteConverter.from_session(groovae_2bar_tap._sess, new_input, new_output)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCRIPTION OF APPROACH (RYAN):\n",
    "#    It should be possible to use the checkpoint file for the trained MusicVAE model to repopulate a tensorflow \n",
    "#    graph and session with all the necessary variables and model structure. In theory, this is exactly what we want to do.\n",
    "#    This methodology, in theory, is ideal because encapsulates the WHOLE MusicVAE model structure and therefore could be\n",
    "#    done to a pre-trained model without any modification to source code. But... it's not working (1/28)\n",
    "\n",
    "# Based on https://stackoverflow.com/questions/56766639/how-to-convert-ckpt-to-pb\n",
    "\n",
    "# Try to load from checkpoint and export to SavedModel based on graph\n",
    "\n",
    "trained_checkpoint_prefix = 'model_checkpoints/groovae_2bar_tap_fixed_velocity/model.ckpt-3668'\n",
    "export_dir = os.path.join('saved_models', 'groovae_2bar_tap_meta')\n",
    "\n",
    "#graph = tf.Graph()\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # Restore from checkpoint\n",
    "    loader = tf.compat.v1.train.import_meta_graph(trained_checkpoint_prefix + '.meta')\n",
    "    loader.restore(sess, trained_checkpoint_prefix)\n",
    "\n",
    "    # Export checkpoint to SavedModel\n",
    "    builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(export_dir)\n",
    "    builder.add_meta_graph_and_variables(sess,\n",
    "                                         [tf.saved_model.TRAINING, tf.saved_model.SERVING],\n",
    "                                         strip_default_attrs=True)\n",
    "    builder.save()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCRIPTION OF APPROACH (RYAN):\n",
    "#    Magenta has a \"arbitrary image stylization\" project that uses a separate model from MusicVAE. I found that this\n",
    "#    project actually has built-in support code to translate the model to TF-Lite. This cell attempts to copy the process\n",
    "#    they use and apply it to the MusicVAE model to see if it will work for us in this context. \n",
    "#    (1/28 Status) Just started this today, have not fully transitioned the \"arbitrary_stylization_image\" code seen below\n",
    "#    to accept the model format and attributes of a MusicVAE model. One major question is what is does a single input look\n",
    "#    like for MusicVAE? \n",
    "\n",
    "# Based on https://github.com/magenta/magenta/blob/eef056b42f09849442158e70e1f0349146d9f94d/magenta/models/arbitrary_image_stylization/arbitrary_image_stylization_convert_tflite.py#L139-L140\n",
    "\n",
    "def load_checkpoint(sess, checkpoint):\n",
    "  \"\"\"Loads a checkpoint file into the session.\n",
    "  Args:\n",
    "    sess: tf.Session, the TF session to load variables from the checkpoint to.\n",
    "    checkpoint: str, path to the checkpoint file.\n",
    "  \"\"\"\n",
    "  model_saver = tf.train.Saver(tf.global_variables())\n",
    "  checkpoint = os.path.expanduser(checkpoint)\n",
    "  if tf.gfile.IsDirectory(checkpoint):\n",
    "    checkpoint = tf.train.latest_checkpoint(checkpoint)\n",
    "    tf.logging.info('loading latest checkpoint file: {}'.format(checkpoint))\n",
    "  model_saver.restore(sess, checkpoint)\n",
    "\n",
    "\n",
    "def export_to_saved_model(checkpoint):\n",
    "  \"\"\"Export arbitrary style transfer trained checkpoints to SavedModel format.\n",
    "  Args:\n",
    "    checkpoint: str, path to the checkpoint file.\n",
    "  Returns:\n",
    "    (str, str) Path to the exported style predict and style transform\n",
    "    SavedModel.\n",
    "  \"\"\"\n",
    "  saved_model_dir = tempfile.mkdtemp()\n",
    "  predict_saved_model_folder = os.path.join(saved_model_dir, 'predict')\n",
    "\n",
    "  with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    # Defines place holder for the style image.\n",
    "    style_image_tensor = tf.placeholder(\n",
    "        tf.float32, shape=[1, None, 27], name='style_image')\n",
    "\n",
    "    # Defines place holder for the content image.\n",
    "    content_image_tensor = tf.placeholder(\n",
    "        tf.float32, shape=[None, None, None, 3], name='content_image')\n",
    "\n",
    "    # Defines the model.\n",
    "    \n",
    "\n",
    "    # Load model weights from  checkpoint file\n",
    "    load_checkpoint(sess, checkpoint)\n",
    "\n",
    "    # Write SavedModel for serving or conversion to TF Lite\n",
    "    tf.saved_model.simple_save(\n",
    "        sess,\n",
    "        predict_saved_model_folder,\n",
    "        inputs={style_image_tensor.name: style_image_tensor},\n",
    "        outputs={'style_bottleneck': bottleneck_feat})\n",
    "\n",
    "  return predict_saved_model_folder\n",
    "\n",
    "\n",
    "def convert_saved_model_to_tflite(saved_model_dir, input_shapes,\n",
    "                                  float_tflite_file, quantized_tflite_file):\n",
    "  \"\"\"Convert SavedModel to TF Lite format.\n",
    "  Also apply weight quantization to generate quantized model\n",
    "  Args:\n",
    "    saved_model_dir: str, path to the SavedModel directory.\n",
    "    input_shapes: dict, input shapes of the SavedModel.\n",
    "    float_tflite_file: str, path to export the float TF Lite model.\n",
    "    quantized_tflite_file: str, path to export the weight quantized TF Lite\n",
    "      model.\n",
    "  Returns: (str, str) Path to the exported style predict and style transform\n",
    "    SavedModel.\n",
    "  \"\"\"\n",
    "\n",
    "  converter = tf.lite.TFLiteConverter.from_saved_model(\n",
    "      saved_model_dir=saved_model_dir, input_shapes=input_shapes)\n",
    "\n",
    "  tflite_float_model = converter.convert()\n",
    "  with tf.gfile.GFile(float_tflite_file, 'wb') as f:\n",
    "    f.write(tflite_float_model)\n",
    "\n",
    "  tf.logging.info('Converted to TF Lite float model: %s; Size: %d KB.' %\n",
    "                  (float_tflite_file, len(tflite_float_model) / 1024))\n",
    "\n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "  tflite_quantize_model = converter.convert()\n",
    "  with tf.gfile.GFile(quantized_tflite_file, 'wb') as f:\n",
    "    f.write(tflite_quantize_model)\n",
    "\n",
    "  tf.logging.info(\n",
    "      'Converted to TF Lite weight quantized model: %s; Size: %d KB.' %\n",
    "      (quantized_tflite_file, len(tflite_quantize_model) / 1024))\n",
    "\n",
    "\n",
    "\n",
    "# Call above functions on trained MusicVAE model etc etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCRIPTION OF APPROACH (RYAN):\n",
    "#    This cell attempts to translate a MultiRNNCell tensorflow object to a SavedModel. \n",
    "#    The MultiRNNCell makes up the Encoder part of the MusicVAE model. The problem with this approach is that\n",
    "#    it does not encompass the entire MusicVAE model. There are tensorflow variables and objects outside of the encoder \n",
    "#    which need to be included in the resulting TF-Lite model. Therefore, I currently (1/28) have no plans to pursure \n",
    "#    this approach unless new discoveries indicate this is the only feasibly way. Not to mention.... it doesn't even work atm\n",
    "\n",
    "from magenta.models.music_vae.lstm_models import BidirectionalLstmEncoder\n",
    "\n",
    "# Initialize Encoder\n",
    "encoder = BidirectionalLstmEncoder()\n",
    "print(encoder)\n",
    "\n",
    "# Build the Encoder using the same parameters from the groovae_2bar_tap config\n",
    "encoder.build(config_2bar_tap.hparams, is_training=False) \n",
    "\n",
    "print(encoder.output_depth)\n",
    "#print(encoder._cells) # tuple of 2 <tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl.MultiRNNCell objects\n",
    "\n",
    "fw_encoder_cells = encoder._cells[0][0] # Get the \"forward\" cell only.\n",
    "print(fw_encoder_cells)\n",
    "\n",
    "# Save MultiRNNCell module to a SavedModel (ONLY HAVE TO RUN THIS ONCE)\n",
    "# tf.saved_model.save(fw_encoder_cells, \"./saved_models/lstm_saved\") \n",
    "\n",
    "#reloaded_encoder = tf.saved_model.load(\"./saved_models/lstm_saved\") \n",
    "#print(reloaded_encoder.signatures)\n",
    "\n",
    "# Translate SavedModel to TF-Lite (CURRENTLY DOES NOT WORK 1/28)\n",
    "# Expects a SignatureKey but our model has none. Have to define our own????\n",
    "converter = tf.compat.v1.lite.TFLiteConverter.from_saved_model(\"./saved_models/lstm_saved\") # path to the SavedModel directory\n",
    "tflite_model = converter.convert()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
