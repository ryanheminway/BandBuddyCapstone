{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "empirical-chair",
   "metadata": {},
   "source": [
    "# Magenta Model Format Investigation\n",
    "\n",
    "In the `MagentaDemo.ipynb` notebook, we demonstrate Magenta's libraries and a pre-trained model. For this model (or similar) to work in our hardware environment, we need the ability to convert it to a TensorFlow-Lite model. Magenta defines their own definition of what a \"model\" is by their \"TrainedModel\" class, so this conversion is not as straight forward as we may desire. This notebook is a sandbox to experiment with methods of converting the Magenta model to a TensorFlow model which can be easily converted to a TensorFlow-Lite.\n",
    "\n",
    "For the following code to successfuly run, you will need to follow the setup listed in `MagentaDemo.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "under-catering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, GrooveLstmDecoder, and hparams:\n",
      "{'max_seq_len': 32, 'z_size': 256, 'free_bits': 48, 'max_beta': 0.2, 'beta_rate': 0.0, 'batch_size': 1, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [256, 256], 'enc_rnn_size': [512], 'dropout_keep_prob': 0.3, 'sampling_schedule': 'constant', 'sampling_rate': 0.0, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False, 'control_preprocessing_rnn_size': [256]}\n",
      "INFO:tensorflow:\n",
      "Encoder Cells (bidirectional):\n",
      "  units: [512]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "INFO:tensorflow:\n",
      "Decoder Cells:\n",
      "  units: [256, 256]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\BB\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
      "C:\\Anaconda\\envs\\BB\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "C:\\Anaconda\\envs\\BB\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unbundling checkpoint.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\RYANHE~1\\AppData\\Local\\Temp\\tmp9hk47kie\\groovae_2bar_tap_fixed_velocity/model.ckpt-3668\n",
      "Config(model=<magenta.models.music_vae.base_model.MusicVAE object at 0x0000028A4278FE08>, hparams=HParams([('batch_size', 512), ('beta_rate', 0.0), ('clip_mode', 'global_norm'), ('conditional', True), ('control_preprocessing_rnn_size', [256]), ('dec_rnn_size', [256, 256]), ('decay_rate', 0.9999), ('dropout_keep_prob', 0.3), ('enc_rnn_size', [512]), ('free_bits', 48), ('grad_clip', 1.0), ('grad_norm_clip_to_zero', 10000), ('learning_rate', 0.001), ('max_beta', 0.2), ('max_seq_len', 32), ('min_learning_rate', 1e-05), ('residual_decoder', False), ('residual_encoder', False), ('sampling_rate', 0.0), ('sampling_schedule', 'constant'), ('use_cudnn', False), ('z_size', 256)]), note_sequence_augmenter=None, data_converter=<magenta.models.music_vae.data.GrooveConverter object at 0x0000028A4278FE88>, train_examples_path=None, eval_examples_path=None, tfds_name='groove/2bar-midionly')\n",
      "\n",
      "\n",
      "[('batch_size', 512), ('beta_rate', 0.0), ('clip_mode', 'global_norm'), ('conditional', True), ('control_preprocessing_rnn_size', [256]), ('dec_rnn_size', [256, 256]), ('decay_rate', 0.9999), ('dropout_keep_prob', 0.3), ('enc_rnn_size', [512]), ('free_bits', 48), ('grad_clip', 1.0), ('grad_norm_clip_to_zero', 10000), ('learning_rate', 0.001), ('max_beta', 0.2), ('max_seq_len', 32), ('min_learning_rate', 1e-05), ('residual_decoder', False), ('residual_encoder', False), ('sampling_rate', 0.0), ('sampling_schedule', 'constant'), ('use_cudnn', False), ('z_size', 256)]\n",
      "\n",
      "\n",
      "[512]\n",
      "<magenta.models.music_vae.trained_model.TrainedModel object at 0x0000028A44E31AC8>\n",
      "<magenta.models.music_vae.base_model.MusicVAE object at 0x0000028A4278FE08>\n",
      "<magenta.models.music_vae.lstm_models.BidirectionalLstmEncoder object at 0x0000028A4278F7C8>\n"
     ]
    }
   ],
   "source": [
    "#@title Setup Environment and Define all helper functionality\n",
    "import tensorflow as tf\n",
    "\n",
    "# General / Math / Sound libraries\n",
    "import copy, warnings, librosa, numpy as np\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Magenta specific stuff\n",
    "import magenta\n",
    "from magenta.models.music_vae import configs\n",
    "from magenta.models.music_vae.trained_model import TrainedModel\n",
    "from magenta.models.music_vae import data\n",
    "\n",
    "# Load model checkpoint\n",
    "GROOVAE_2BAR_TAP_FIXED_VELOCITY = \"model_checkpoints/groovae_2bar_tap_fixed_velocity.tar\"\n",
    "config_2bar_tap = configs.CONFIG_MAP['groovae_2bar_tap_fixed_velocity']\n",
    "# Create a TrainedModel (Magenta class) from config and checkpoint\n",
    "# The config specifies the type of the model, and their TrainedModel class constructs the\n",
    "#   appropriate back-end tensorflow graph for that specific model\n",
    "groovae_2bar_tap = TrainedModel(config_2bar_tap, 1, checkpoint_dir_or_path=GROOVAE_2BAR_TAP_FIXED_VELOCITY)\n",
    "\n",
    "print(config_2bar_tap)\n",
    "print(\"\\n\")\n",
    "print(config_2bar_tap.hparams)\n",
    "print(\"\\n\")\n",
    "print(config_2bar_tap.hparams.enc_rnn_size)\n",
    "print(groovae_2bar_tap) # magenta.models.music_vae.trained_model.TrainedModel object\n",
    "print(config_2bar_tap.model) # magenta.models.music_vae.base_model.MusicVAE\n",
    "print(config_2bar_tap.model.encoder) # magenta.models.music_vae.lstm_models.BidirectionalLstmEncoder \n",
    "#print(config_2bar_tap.model.encoder._cells) # THIS GIVES AN ERROR.... BUT WHY???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-playing",
   "metadata": {},
   "source": [
    "# Initial Conclusions and Next Steps\n",
    "\n",
    "It does not appear that you can access the underlying TensorFlow graphs directly from the library code that is exposed in our `MagentaDemo.ipynb`. So instead, I dug into the code that makes up that library. I found that their `TrainedModel` class wraps around a multitude of model types. For this case, they have defined a `MusicVAE` model. This model is really another wrapper around two internal neural networks. One of the networks is the \"encoder\" and the other is the \"decoder\". The encoder / decoder structure is essential to how a VAE (Variational Auto-Encoder) operates. The encoder and decoder are the neural network structures which get constructed using TensorFlow elements. This is the component we want to translate to use TF-Lite. The following code is an effort to reproduce the TensorFlow structure of what their encoder would look like. There will be some more work to figure out how to use all their library code considering it does not expose what we need... but reproducing what they have in the back-end is a good start.\n",
    "\n",
    "I found this all by digging through https://github.com/magenta/magenta/tree/2d0fd456d7faa272733b57d286f5f26998082cf8/magenta/models/music_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "attended-shock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<magenta.models.music_vae.lstm_models.BidirectionalLstmEncoder object at 0x0000028A4443C548>\n",
      "INFO:tensorflow:\n",
      "Encoder Cells (bidirectional):\n",
      "  units: [512]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "<tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl.MultiRNNCell object at 0x0000028A45A846C8>\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "We could not automatically infer the static shape of the layer's output. Please implement the `compute_output_shape` method on your layer (MultiRNNCell).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda\\envs\\BB\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    754\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __call__() got an unexpected keyword argument 'training'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-bea643d53233>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mfw_encoder_cells\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cells\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfw_encoder_cells\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfw_encoder_cells\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Save MultiRNNCell module to a SavedModel (ONLY HAVE TO RUN THIS ONCE)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\BB\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    760\u001b[0m                   \u001b[1;34m'layer\\'s output. Please implement the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m                   \u001b[1;34m'`compute_output_shape` method on your layer (%s).'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m                   self.__class__.__name__), e)\n\u001b[0m\u001b[0;32m    763\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     raise NotImplementedError(\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\BB\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: We could not automatically infer the static shape of the layer's output. Please implement the `compute_output_shape` method on your layer (MultiRNNCell)."
     ]
    }
   ],
   "source": [
    "from magenta.models.music_vae.lstm_models import BidirectionalLstmEncoder\n",
    "\n",
    "# Initialize Encoder\n",
    "encoder = BidirectionalLstmEncoder()\n",
    "print(encoder)\n",
    "\n",
    "# Build the Encoder using the same parameters from the groovae_2bar_tap config\n",
    "# Notice we are NOT training\n",
    "encoder.build(config_2bar_tap.hparams, is_training=False) \n",
    "# Notice the output this gives is also present in the output from the code above. \n",
    "# Hopefully this is enough confidence that the same code is running behind the scenes.\n",
    "\n",
    "#print(encoder._cells) # tuple of 2 <tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl.MultiRNNCell objects\n",
    "\n",
    "fw_encoder_cells = encoder._cells[0][0]\n",
    "print(fw_encoder_cells)\n",
    "\n",
    "# Save MultiRNNCell module to a SavedModel (ONLY HAVE TO RUN THIS ONCE)\n",
    "# tf.saved_model.save(fw_encoder_cells, \"./saved_models/lstm_saved\") \n",
    "\n",
    "#reloaded_encoder = tf.saved_model.load(\"./saved_models/lstm_saved\") \n",
    "#print(reloaded_encoder.signatures)\n",
    "\n",
    "# Translate SavedModel to TF-Lite (CURRENTLY DOES NOT WORK 1/26 6:21 PM)\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./saved_models/lstm_saved\", signature_keys=None) # path to the SavedModel directory\n",
    "tflite_model = converter.convert()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
